# ğŸ—„ï¸ Twitter Scraper Pro - æœ¬åœ°æ•°æ®åº“å®Œæ•´æ­å»ºæŒ‡å—

> æœ¬æŒ‡å—æ•™ä½ å¦‚ä½•æ­å»ºå®Œæ•´çš„æœ¬åœ°æ•°æ®æŒä¹…åŒ–ç³»ç»Ÿ  
> ğŸ“¦ æ”¯æŒï¼šKuzuDB å›¾æ•°æ®åº“ + è‡ªåŠ¨åŒæ­¥ + å†å²æŸ¥è¯¢

---

## ğŸ“‹ ç›®å½•

1. [æ–¹æ¡ˆé€‰æ‹©](#æ–¹æ¡ˆé€‰æ‹©)
2. [æ–¹æ¡ˆ Aï¼šä»… Chrome æ‰©å±•ï¼ˆé›¶é…ç½®ï¼‰](#æ–¹æ¡ˆ-a ä»… chrome æ‰©å±•é›¶é…ç½®)
3. [æ–¹æ¡ˆ Bï¼šChrome æ‰©å±• + Python åç«¯ï¼ˆæ¨èï¼‰](#æ–¹æ¡ˆ-bchrome æ‰©å±•--python-åç«¯æ¨è)
4. [æ–¹æ¡ˆ Cï¼šDocker éƒ¨ç½²ï¼ˆé«˜çº§ï¼‰](#æ–¹æ¡ˆ-cdocker éƒ¨ç½²é«˜çº§)
5. [æ•°æ®åº“ç®¡ç†](#æ•°æ®åº“ç®¡ç†)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ¯ æ–¹æ¡ˆé€‰æ‹©

| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ | æ•°æ®å®¹é‡ |
|------|----------|------|----------|
| **æ–¹æ¡ˆ A** | ä¸´æ—¶ä½¿ç”¨ã€è½»åº¦ç”¨æˆ· | â­ ç®€å• | 10MB (Chrome Storage) |
| **æ–¹æ¡ˆ B** | æ—¥å¸¸ä½¿ç”¨ã€ç ”ç©¶äººå‘˜ | â­â­ ä¸­ç­‰ | æ— é™ (KuzuDB) |
| **æ–¹æ¡ˆ C** | å›¢é˜Ÿä½¿ç”¨ã€ç”Ÿäº§ç¯å¢ƒ | â­â­â­ å¤æ‚ | æ— é™ + å¤šç”¨æˆ· |

---

## æ–¹æ¡ˆ Aï¼šä»… Chrome æ‰©å±•ï¼ˆé›¶é…ç½®ï¼‰

### é€‚ç”¨äººç¾¤
- âœ… ä¸æƒ³å®‰è£… Python
- âœ… åªéœ€è¦ä¸´æ—¶æŠ“å–
- âœ… æ•°æ®é‡å°ï¼ˆ<1000 æ¡ï¼‰

### å®‰è£…æ­¥éª¤

#### 1. æ‰“å¼€ Chrome æ‰©å±•ç®¡ç†é¡µ
```
chrome://extensions/
```

#### 2. å¼€å¯å¼€å‘è€…æ¨¡å¼
ç‚¹å‡»å³ä¸Šè§’å¼€å…³

#### 3. åŠ è½½æ‰©å±•
- ç‚¹å‡»"åŠ è½½å·²è§£å‹çš„æ‰©å±•ç¨‹åº"
- é€‰æ‹© `dist` æ–‡ä»¶å¤¹
- æ‰©å±•å›¾æ ‡å‡ºç°åœ¨å·¥å…·æ 

#### 4. é…ç½® API Key
- ç‚¹å‡»æ‰©å±•å›¾æ ‡
- è¿›å…¥"è®¾ç½®"æ ‡ç­¾
- è¾“å…¥ [æ™ºè°± API Key](https://open.bigmodel.cn/api-keys)
- ä¿å­˜

#### 5. å¼€å§‹ä½¿ç”¨
- è®¿é—® twitter.com æˆ– reddit.com
- å¼€å§‹æµè§ˆï¼Œè‡ªåŠ¨æŠ“å–

### æ•°æ®ç®¡ç†

#### æŸ¥çœ‹æœ¬åœ°æ•°æ®
```javascript
// åœ¨æ‰©å±•å¼¹çª—æ§åˆ¶å°æ‰§è¡Œ
const data = await chrome.storage.local.get(['posts', 'filteredPosts']);
console.log(`æœ¬åœ°å¸–å­æ•°ï¼š${data.posts?.length || 0}`);
console.log(`ç­›é€‰åå¸–å­æ•°ï¼š${data.filteredPosts?.length || 0}`);
```

#### å¯¼å‡ºæ•°æ®
1. ç‚¹å‡»æ‰©å±•å›¾æ ‡
2. ç‚¹å‡»"ğŸ“¥ å¯¼å‡ºå…¨éƒ¨"
3. è‡ªåŠ¨ä¸‹è½½ CSV æ–‡ä»¶

#### æ¸…ç†æ•°æ®
1. ç‚¹å‡»æ‰©å±•å›¾æ ‡
2. è¿›å…¥"å…¨éƒ¨å¸–å­"æ ‡ç­¾
3. ç‚¹å‡»"æ¸…ç©º"æŒ‰é’®

### é™åˆ¶
- âš ï¸ æœ€å¤šå­˜å‚¨ ~10MB æ•°æ®
- âš ï¸ æ¸…é™¤æ‰©å±•ä¼šä¸¢å¤±æ•°æ®
- âš ï¸ æ— æ³•è·¨è®¾å¤‡åŒæ­¥

---

## æ–¹æ¡ˆ Bï¼šChrome æ‰©å±• + Python åç«¯ï¼ˆæ¨èï¼‰

### é€‚ç”¨äººç¾¤
- âœ… æ—¥å¸¸é¢‘ç¹ä½¿ç”¨
- âœ… éœ€è¦æ•°æ®æŒä¹…åŒ–
- âœ… éœ€è¦å†å²æŸ¥è¯¢
- âœ… æ•°æ®é‡å¤§ï¼ˆ>1000 æ¡ï¼‰

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- æ“ä½œç³»ç»Ÿï¼šWindows/Mac/Linux
- å†…å­˜ï¼šæœ€å°‘ 512MB

### å®Œæ•´å®‰è£…æ­¥éª¤

#### æ­¥éª¤ 1ï¼šå®‰è£… Python

**Windows:**
```powershell
# ä»å®˜ç½‘ä¸‹è½½
# https://www.python.org/downloads/
# å®‰è£…æ—¶å‹¾é€‰ "Add Python to PATH"
```

**Mac:**
```bash
brew install python@3.11
```

**Linux (Ubuntu/Debian):**
```bash
sudo apt update
sudo apt install python3 python3-pip
```

**éªŒè¯å®‰è£…:**
```bash
python --version
# åº”è¯¥æ˜¾ç¤ºï¼šPython 3.8.x æˆ–æ›´é«˜
```

#### æ­¥éª¤ 2ï¼šå…‹éš†é¡¹ç›®

```bash
# å…‹éš† GitHub ä»“åº“
git clone https://github.com/LegionZYX/twitter-scraper-pro.git
cd twitter-scraper-pro/twitter-scraper-extension-v2
```

#### æ­¥éª¤ 3ï¼šå®‰è£… Python ä¾èµ–

```bash
cd backend
pip install -r requirements.txt
```

**å¦‚æœ pip å®‰è£…æ…¢ï¼ˆå›½å†…ç”¨æˆ·ï¼‰:**
```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**ä¾èµ–è¯´æ˜:**
```
kuzu==0.5.0     # å›¾æ•°æ®åº“ï¼ˆæ ¸å¿ƒï¼‰
```

#### æ­¥éª¤ 4ï¼šåˆå§‹åŒ–æ•°æ®åº“

```bash
cd backend
python -c "
import asyncio
from database import SocialScraperKG

async def init():
    kg = SocialScraperKG('./database/twitter_scraper')
    await kg.init()
    print('âœ“ æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ')
    stats = await kg.get_stats()
    print(f'æ•°æ®åº“è·¯å¾„ï¼š./database/twitter_scraper')
    print(f'åˆå§‹ç»Ÿè®¡ï¼š{stats}')
    await kg.close()

asyncio.run(init())
"
```

**è¾“å‡ºç¤ºä¾‹:**
```
âœ“ æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ
æ•°æ®åº“è·¯å¾„ï¼š./database/twitter_scraper
åˆå§‹ç»Ÿè®¡ï¼š{'posts': 0, 'filtered_posts': 0, 'discovery_results': 0, 'archived_posts': 0}
```

#### æ­¥éª¤ 5ï¼šå¯åŠ¨åç«¯æœåŠ¡

```bash
# åŸºç¡€å¯åŠ¨
python server_minimal.py --port 8770

# å¸¦æ—¥å¿—è¾“å‡º
python server_minimal.py --port 8770 2>&1 | tee server.log

# åå°è¿è¡Œï¼ˆLinux/Macï¼‰
nohup python server_minimal.py --port 8770 > server.log 2>&1 &

# Windows åå°è¿è¡Œ
start /B python server_minimal.py --port 8770
```

**æˆåŠŸè¾“å‡º:**
```
18:30:45 [INFO] Initializing database at ./database/twitter_scraper...
18:30:46 [INFO] Database initialized
18:30:46 [INFO] Starting server on 127.0.0.1:8770
18:30:46 [OK] Server running - http://127.0.0.1:8770
```

#### æ­¥éª¤ 6ï¼šéªŒè¯åç«¯

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8770/health

# æˆ–è€…åœ¨æµè§ˆå™¨è®¿é—®
# http://localhost:8770/health
```

**å“åº”ç¤ºä¾‹:**
```json
{
  "status": "healthy",
  "database": "connected",
  "stats": {
    "posts": 0,
    "filtered_posts": 0,
    "discovery_results": 0
  }
}
```

#### æ­¥éª¤ 7ï¼šé…ç½® Chrome æ‰©å±•

1. **åŠ è½½æ‰©å±•**
   ```
   chrome://extensions/
   â†’ å¼€å‘è€…æ¨¡å¼
   â†’ åŠ è½½å·²è§£å‹çš„æ‰©å±•ç¨‹åº
   â†’ é€‰æ‹© dist æ–‡ä»¶å¤¹
   ```

2. **è‡ªåŠ¨åŒæ­¥**
   - æ‰©å±•ä¼šæ¯ 5 åˆ†é’Ÿè‡ªåŠ¨æ£€æŸ¥åç«¯
   - åç«¯å¯ç”¨æ—¶è‡ªåŠ¨åŒæ­¥æ•°æ®
   - æ— éœ€æ‰‹åŠ¨é…ç½®

3. **éªŒè¯è¿æ¥**
   - ç‚¹å‡»æ‰©å±•å›¾æ ‡
   - æŸ¥çœ‹çŠ¶æ€æŒ‡ç¤ºå™¨
   - åº”è¯¥æ˜¾ç¤º"ğŸŸ¢ åœ¨çº¿"

---

## æ–¹æ¡ˆ Cï¼šDocker éƒ¨ç½²ï¼ˆé«˜çº§ï¼‰

### é€‚ç”¨äººç¾¤
- âœ… å›¢é˜Ÿä½¿ç”¨
- âœ… éœ€è¦å¤šç”¨æˆ·
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ç³»ç»Ÿè¦æ±‚
- Docker 20.10+
- Docker Compose 2.0+
- å†…å­˜ï¼šæœ€å°‘ 1GB

### åˆ›å»º Dockerfile

**æ–‡ä»¶ï¼š** `backend/Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# åˆ›å»ºæ•°æ®åº“ç›®å½•
RUN mkdir -p /app/database/twitter_scraper

# æš´éœ²ç«¯å£
EXPOSE 8770

# å¯åŠ¨å‘½ä»¤
CMD ["python", "server_minimal.py", "--host", "0.0.0.0", "--port", "8770"]
```

### åˆ›å»º docker-compose.yml

**æ–‡ä»¶ï¼š** `docker-compose.yml`

```yaml
version: '3.8'

services:
  twitter-scraper-backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8770:8770"
    volumes:
      - ./backend/database:/app/database
    restart: unless-stopped
    environment:
      - LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8770/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### å¯åŠ¨æœåŠ¡

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# éªŒè¯è¿è¡Œ
curl http://localhost:8770/health
```

### ç®¡ç†å‘½ä»¤

```bash
# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps

# è¿›å…¥å®¹å™¨
docker-compose exec twitter-scraper-backend bash
```

---

## ğŸ—„ï¸ æ•°æ®åº“ç®¡ç†

### æ•°æ®å¤‡ä»½

#### æ–¹æ³• 1ï¼šæ‰‹åŠ¨å¤‡ä»½

```bash
# åœæ­¢åç«¯æœåŠ¡
# å¤åˆ¶æ•°æ®åº“ç›®å½•
cp -r backend/database/twitter_scraper /backup/location/twitter_scraper_$(date +%Y%m%d)

# æˆ–è€…å‹ç¼©å¤‡ä»½
tar -czf twitter_scraper_backup_$(date +%Y%m%d).tar.gz backend/database/twitter_scraper
```

#### æ–¹æ³• 2ï¼šè‡ªåŠ¨å¤‡ä»½è„šæœ¬

**æ–‡ä»¶ï¼š** `scripts/backup.sh` (Linux/Mac)

```bash
#!/bin/bash

BACKUP_DIR="/backup/twitter_scraper"
DATE=$(date +%Y%m%d_%H%M%S)
DB_PATH="./backend/database/twitter_scraper"

mkdir -p $BACKUP_DIR

echo "å¼€å§‹å¤‡ä»½æ•°æ®åº“..."
tar -czf $BACKUP_DIR/backup_$DATE.tar.gz $DB_PATH

if [ $? -eq 0 ]; then
    echo "âœ“ å¤‡ä»½æˆåŠŸï¼š$BACKUP_DIR/backup_$DATE.tar.gz"
    
    # æ¸…ç† 7 å¤©å‰çš„å¤‡ä»½
    find $BACKUP_DIR -name "backup_*.tar.gz" -mtime +7 -delete
    echo "âœ“ å·²æ¸…ç†æ—§å¤‡ä»½"
else
    echo "âœ— å¤‡ä»½å¤±è´¥"
    exit 1
fi
```

**è®¾ç½®å®šæ—¶ä»»åŠ¡:**
```bash
# ç¼–è¾‘ crontab
crontab -e

# æ·»åŠ æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½
0 2 * * * /path/to/backup.sh
```

### æ•°æ®æ¢å¤

```bash
# åœæ­¢åç«¯æœåŠ¡
# è§£å‹å¤‡ä»½
tar -xzf twitter_scraper_backup_20260225.tar.gz -C ./backend/database/

# é‡å¯åç«¯
python server_minimal.py --port 8770
```

### æ•°æ®æŸ¥è¯¢

#### ä½¿ç”¨ Python è„šæœ¬æŸ¥è¯¢

**æ–‡ä»¶ï¼š** `scripts/query_posts.py`

```python
import asyncio
import sys
sys.path.insert(0, '../backend')

from database import SocialScraperKG

async def query():
    kg = SocialScraperKG('../backend/database/twitter_scraper')
    await kg.init()
    
    # æŸ¥è¯¢æœ€è¿‘ 24 å°æ—¶çš„å¸–å­
    posts = await kg.get_recent_posts(hours=24, limit=10)
    
    print(f"æ‰¾åˆ° {len(posts)} æ¡å¸–å­:")
    for post in posts:
        print(f"- {post['author']}: {post['content'][:50]}...")
    
    # è·å–ç»Ÿè®¡
    stats = await kg.get_stats()
    print(f"\næ•°æ®åº“ç»Ÿè®¡:")
    print(f"  æ€»å¸–å­æ•°ï¼š{stats['posts']}")
    print(f"  ç­›é€‰åï¼š{stats['filtered_posts']}")
    print(f"  å·²å½’æ¡£ï¼š{stats['archived_posts']}")
    
    await kg.close()

asyncio.run(query())
```

**è¿è¡Œ:**
```bash
cd scripts
python query_posts.py
```

### æ€§èƒ½ä¼˜åŒ–

#### 1. æ‰¹é‡æ“ä½œ

```python
# å¥½çš„åšæ³•
await kg.add_posts_batch(posts)

# ä¸å¥½çš„åšæ³•
for post in posts:
    await kg.add_post(post)
```

#### 2. å®šæœŸæ¸…ç†

```python
# æ·»åŠ è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
from scheduler import add_cleanup_job

add_cleanup_job(
    days_to_keep=90,  # ä¿ç•™ 90 å¤©
    min_relevance_score=3  # æœ€ä½ç›¸å…³åº¦
)
```

#### 3. ç´¢å¼•ä¼˜åŒ–

KuzuDB è‡ªåŠ¨ä¸º PRIMARY KEY åˆ›å»ºç´¢å¼•ã€‚

å¦‚éœ€æ·»åŠ è‡ªå®šä¹‰ç´¢å¼•ï¼š
```python
# åœ¨ schema.py ä¸­æ·»åŠ 
CREATE NODE INDEX ON Post(scrapedAt)
CREATE NODE INDEX ON FilteredPost(category)
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šç«¯å£è¢«å ç”¨

**é”™è¯¯:**
```
OSError: [WinError 10048] Only one usage of each socket address is normally permitted
```

**è§£å†³:**
```bash
# Windowsï¼šæŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
netstat -ano | findstr :8770

# æ€æ­»è¿›ç¨‹
taskkill /F /PID <PID>

# æˆ–è€…ä½¿ç”¨å…¶ä»–ç«¯å£
python server_minimal.py --port 8771
```

### é—®é¢˜ 2ï¼šKuzuDB åˆå§‹åŒ–å¤±è´¥

**é”™è¯¯:**
```
Exception: Failed to open database
```

**è§£å†³:**
```bash
# åˆ é™¤æŸåçš„æ•°æ®åº“
rm -rf backend/database/twitter_scraper

# é‡æ–°åˆå§‹åŒ–
python -c "
import asyncio
from database import SocialScraperKG
async def init():
    kg = SocialScraperKG('./database/twitter_scraper')
    await kg.init()
    print('âœ“ æ•°æ®åº“é‡æ–°åˆå§‹åŒ–æˆåŠŸ')
    await kg.close()
asyncio.run(init())
"
```

### é—®é¢˜ 3ï¼šChrome æ‰©å±•æ— æ³•è¿æ¥åç«¯

**æ£€æŸ¥:**
```bash
# éªŒè¯åç«¯è¿è¡Œ
curl http://localhost:8770/health

# æ£€æŸ¥é˜²ç«å¢™
# Windows: å…è®¸ Python é€šè¿‡é˜²ç«å¢™
# Mac/Linux: sudo ufw allow 8770
```

**è§£å†³:**
1. ç¡®ä¿åç«¯æ­£åœ¨è¿è¡Œ
2. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
3. é‡å¯ Chrome æµè§ˆå™¨

### é—®é¢˜ 4ï¼šä¾èµ–å®‰è£…å¤±è´¥

**é”™è¯¯:**
```
ERROR: Could not find a version that satisfies the requirement kuzu
```

**è§£å†³:**
```bash
# å‡çº§ pip
python -m pip install --upgrade pip

# ä½¿ç”¨å›½å†…é•œåƒ
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# æˆ–è€…æ‰‹åŠ¨å®‰è£… kuzu
pip install kuzu==0.5.0
```

### é—®é¢˜ 5ï¼šæ•°æ®ä¸åŒæ­¥

**æ£€æŸ¥åç«¯æ—¥å¿—:**
```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f backend/server.log

# æˆ–è€…å®æ—¶æŸ¥çœ‹
python server_minimal.py --port 8770 2>&1 | grep -E "(INFO|ERROR|OK)"
```

**æ‰‹åŠ¨è§¦å‘åŒæ­¥:**
1. ç‚¹å‡»æ‰©å±•å›¾æ ‡
2. æŸ¥çœ‹åŒæ­¥çŠ¶æ€
3. ç‚¹å‡»"ç«‹å³åŒæ­¥"æŒ‰é’®

---

## ğŸ“Š æ•°æ®åº“ç›‘æ§

### æŸ¥çœ‹æ•°æ®åº“å¤§å°

```bash
# Linux/Mac
du -sh backend/database/twitter_scraper

# Windows
Get-ChildItem backend\database\twitter_scraper -Recurse | Measure-Object -Property Length -Sum
```

### æŸ¥çœ‹å¸–å­æ•°é‡

```bash
curl http://localhost:8770/api/stats | python -m json.tool
```

### è®¾ç½®å‘Šè­¦

**è„šæœ¬ï¼š** `scripts/monitor.py`

```python
import requests
import smtplib
from email.mime.text import MIMEText

def check_health():
    try:
        response = requests.get('http://localhost:8770/health', timeout=5)
        if response.ok:
            print('âœ“ åç«¯å¥åº·')
            return True
        else:
            send_alert('åç«¯å“åº”å¼‚å¸¸')
            return False
    except:
        send_alert('åç«¯æ— æ³•è¿æ¥')
        return False

def send_alert(message):
    # å‘é€é‚®ä»¶/Telegram/é£ä¹¦é€šçŸ¥
    print(f'âš ï¸ å‘Šè­¦ï¼š{message}')

if __name__ == '__main__':
    check_health()
```

**å®šæ—¶è¿è¡Œ:**
```bash
# æ¯ 5 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
*/5 * * * * python scripts/monitor.py
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [README.md](README.md) - ä¸»æ–‡æ¡£
- [INDEPENDENT_DEPLOYMENT.md](INDEPENDENT_DEPLOYMENT.md) - ç‹¬ç«‹éƒ¨ç½²æŒ‡å—
- [BACKEND_GUIDE.md](BACKEND_GUIDE.md) - åç«¯ä½¿ç”¨æŒ‡å—
- [GITHUB_PUSH_GUIDE.md](GITHUB_PUSH_GUIDE.md) - GitHub æ¨é€æŒ‡å—

---

## ğŸ¤ è·å–å¸®åŠ©

- **GitHub Issues:** æŠ¥å‘Š Bug
- **GitHub Discussions:** ä½¿ç”¨é—®é¢˜
- **æ–‡æ¡£:** æŸ¥çœ‹ç›¸å…³æ–‡æ¡£

---

**æœ€åæ›´æ–°ï¼š** 2026-02-27  
**ç»´æŠ¤è€…ï¼š** Twitter Scraper Team
