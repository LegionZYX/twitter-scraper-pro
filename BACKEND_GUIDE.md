# Twitter Scraper Pro v2.2 - Python åç«¯ä½¿ç”¨æŒ‡å—

> ğŸ“¦ å¯é€‰ç»„ä»¶ - ç”¨äºæ•°æ®æŒä¹…åŒ–å’Œè‡ªåŠ¨åŒæ­¥  
> ğŸš€ æç®€è®¾è®¡ - ä»… 1 ä¸ªä¾èµ–ï¼ˆkuzuï¼‰

---

## ğŸ¯ åç«¯ä½œç”¨

### æ ¸å¿ƒåŠŸèƒ½
1. **æ•°æ®æŒä¹…åŒ–** - KuzuDB å­˜å‚¨ï¼ˆæ— å®¹é‡é™åˆ¶ï¼‰
2. **è‡ªåŠ¨åŒæ­¥** - æ¯ 5 åˆ†é’Ÿæ£€æŸ¥å¹¶åŒæ­¥
3. **å†å²æŸ¥è¯¢** - æŸ¥è¯¢ä»»æ„æ—¶é—´æ®µæ•°æ®
4. **æ‰¹é‡å¯¼å‡º** - å¯¼å‡ºä¸º CSV/JSON

### ä¸ Chrome æ‰©å±•çš„å…³ç³»
```
Chrome æ‰©å±•ï¼ˆå¿…é¡»ï¼‰          Python åç«¯ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ å®æ—¶æŠ“å–                 â”œâ”€â”€ æŒä¹…åŒ–å­˜å‚¨
â”œâ”€â”€ æœ¬åœ°ç¼“å­˜                 â”œâ”€â”€ å†å²æ•°æ®
â”œâ”€â”€ UI å±•ç¤º                   â”œâ”€â”€ æ•°æ®åˆ†æ
â””â”€â”€ ç¦»çº¿ä½¿ç”¨                 â””â”€â”€ è‡ªåŠ¨åŒæ­¥
```

**æ— åç«¯ä¹Ÿèƒ½å®Œå…¨ä½¿ç”¨ï¼** åç«¯æ˜¯å¯é€‰çš„å¢å¼ºåŠŸèƒ½ã€‚

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### 1. å®‰è£…ä¾èµ–

```bash
cd backend
pip install -r requirements.txt
```

**ä¾èµ–è¯´æ˜ï¼š**
```
kuzu==0.5.0     # å›¾æ•°æ®åº“ï¼ˆå”¯ä¸€ä¾èµ–ï¼‰
```

### 2. å¯åŠ¨æœåŠ¡

```bash
python server_minimal.py --port 8770
```

**è¾“å‡ºï¼š**
```
18:30:45 [INFO] Initializing database at ./database/twitter_scraper...
18:30:46 [INFO] Database initialized
18:30:46 [INFO] Starting server on 127.0.0.1:8770
18:30:46 [OK] Server running - http://127.0.0.1:8770
```

### 3. éªŒè¯è¿è¡Œ

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8770/health

# è¿”å›ï¼š
{
  "status": "healthy",
  "database": "connected",
  "stats": {
    "posts": 0,
    "filtered_posts": 0,
    "discovery_results": 0
  }
}
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### å‘½ä»¤è¡Œå‚æ•°

```bash
python server_minimal.py \
  --host 127.0.0.1 \
  --port 8770 \
  --db-path ./database/twitter_scraper
```

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--host` | 127.0.0.1 | ç›‘å¬åœ°å€ |
| `--port` | 8770 | ç›‘å¬ç«¯å£ |
| `--db-path` | ./database/twitter_scraper | æ•°æ®åº“è·¯å¾„ |

### ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

åˆ›å»º `backend/.env` æ–‡ä»¶ï¼š

```bash
KUZU_DB_PATH=./data/kuzu
LOG_LEVEL=INFO
```

---

## ğŸ“¡ API ç«¯ç‚¹

### GET /health

å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8770/health
```

**å“åº”ï¼š**
```json
{
  "status": "healthy",
  "database": "connected",
  "stats": {...}
}
```

---

### GET /api/stats

è·å–ç»Ÿè®¡æ•°æ®

```bash
curl http://localhost:8770/api/stats
```

**å“åº”ï¼š**
```json
{
  "posts": 150,
  "filtered_posts": 45,
  "discovery_results": 45,
  "archived_posts": 0,
  "sentiments": {
    "positive": 20,
    "negative": 5,
    "neutral": 20
  }
}
```

---

### GET /api/posts

è·å–å¸–å­åˆ—è¡¨

```bash
curl "http://localhost:8770/api/posts?hours=24&limit=100"
```

**å‚æ•°ï¼š**
- `hours` - æœ€è¿‘ N å°æ—¶ï¼ˆé»˜è®¤ 24ï¼‰
- `limit` - æœ€å¤§æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
- `platform` - å¹³å°è¿‡æ»¤ï¼ˆtwitter/redditï¼‰

**å“åº”ï¼š**
```json
{
  "posts": [...],
  "count": 100
}
```

---

### GET /api/posts/filtered

è·å–ç­›é€‰åçš„å¸–å­

```bash
curl "http://localhost:8770/api/posts/filtered?category=tech&limit=50"
```

**å‚æ•°ï¼š**
- `category` - åˆ†ç±»è¿‡æ»¤
- `limit` - æœ€å¤§æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰

---

### POST /api/posts/batch

æ‰¹é‡ä¸Šä¼ å¸–å­

```bash
curl -X POST http://localhost:8770/api/posts/batch \
  -H "Content-Type: application/json" \
  -d '{
    "posts": [...],
    "filtered": [...]
  }'
```

**å“åº”ï¼š**
```json
{
  "status": "success",
  "posts_stored": 50,
  "filtered_stored": 15,
  "discovery_stored": 15
}
```

---

### POST /api/cleanup/run

æ‰‹åŠ¨è§¦å‘æ¸…ç†

```bash
curl -X POST http://localhost:8770/api/cleanup/run \
  -H "Content-Type: application/json" \
  -d '{"dry_run": true}'
```

**å‚æ•°ï¼š**
- `dry_run` - é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…åˆ é™¤ï¼‰

---

## ğŸ’¾ æ•°æ®ç®¡ç†

### æ•°æ®åº“ä½ç½®

```
twitter-scraper-extension-v2/backend/database/twitter_scraper/
```

### å¤‡ä»½æ•°æ®åº“

```bash
# å¤åˆ¶æ•´ä¸ªæ•°æ®åº“ç›®å½•
cp -r backend/database/twitter_scraper /backup/location/
```

### æ¢å¤æ•°æ®åº“

```bash
# åœæ­¢åç«¯
# å¤åˆ¶å¤‡ä»½åˆ°åŸä½ç½®
cp -r /backup/location/twitter_scraper backend/database/
# é‡å¯åç«¯
```

### æ¸…ç©ºæ•°æ®

```bash
# åˆ é™¤æ•°æ®åº“ç›®å½•
rm -rf backend/database/twitter_scraper
# é‡å¯åç«¯ä¼šè‡ªåŠ¨åˆ›å»º
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

### æŸ¥çœ‹æ—¥å¿—

åç«¯é»˜è®¤è¾“å‡ºæ—¥å¿—åˆ°æ§åˆ¶å°ï¼š

```
18:30:45 [INFO] Initializing database...
18:30:46 [OK] Database initialized
18:35:00 [INFO] Received batch: 50 posts
18:35:01 [OK] Batch stored: 50 posts, 15 filtered, 15 discovery
```

### é‡å®šå‘æ—¥å¿—åˆ°æ–‡ä»¶

```bash
python server_minimal.py --port 8770 > server.log 2>&1
```

### æŸ¥çœ‹å®æ—¶æ—¥å¿—

```bash
tail -f server.log
```

---

## ğŸ”„ ä¸ Chrome æ‰©å±•ç¤ºåŒæ­¥

### è‡ªåŠ¨åŒæ­¥æœºåˆ¶

1. **Chrome æ‰©å±•**æ¯ 5 åˆ†é’Ÿæ£€æŸ¥åç«¯çŠ¶æ€
2. **åç«¯å¯ç”¨**æ—¶å‘é€å¾…åŒæ­¥æ•°æ®
3. **åç«¯ä¸å¯ç”¨**æ—¶ç»§ç»­ä½¿ç”¨æœ¬åœ°ç¼“å­˜
4. **åç«¯æ¢å¤**åè‡ªåŠ¨åŒæ­¥æ‰€æœ‰å¾…å¤„ç†æ•°æ®

### æ‰‹åŠ¨è§¦å‘åŒæ­¥

åœ¨ Chrome æ‰©å±•ä¸­ï¼š
1. ç‚¹å‡»æ‰©å±•å›¾æ ‡
2. æŸ¥çœ‹åŒæ­¥çŠ¶æ€
3. ç‚¹å‡»"ç«‹å³åŒæ­¥"æŒ‰é’®

---

## ğŸ§ª æµ‹è¯•åç«¯

### æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–

```bash
python -c "
import sys, os
sys.path.insert(0, '.')
os.environ['KUZU_DB_PATH'] = './database/test'
from database import SocialScraperKG
import asyncio

async def test():
    kg = SocialScraperKG('./database/test')
    await kg.init()
    stats = await kg.get_stats()
    print(f'Database initialized: {stats}')
    await kg.close()

asyncio.run(test())
"
```

### æµ‹è¯• API ç«¯ç‚¹

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8770/health

# è·å–ç»Ÿè®¡
curl http://localhost:8770/api/stats

# å‘é€æµ‹è¯•æ•°æ®
curl -X POST http://localhost:8770/api/posts/batch \
  -H "Content-Type: application/json" \
  -d '{
    "posts": [{
      "id": "test_1",
      "platform": "twitter",
      "author": "test_user",
      "content": "Test post content",
      "url": "https://twitter.com/test/status/1",
      "timestamp": "2026-02-25T10:00:00Z",
      "score": 100,
      "replies": 10,
      "media": [],
      "scrapedAt": 1708851600000
    }]
  }'
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šç«¯å£è¢«å ç”¨

**é”™è¯¯ï¼š**
```
OSError: [WinError 10048] Only one usage of each socket address is normally permitted
```

**è§£å†³ï¼š**
```bash
# Windowsï¼šæŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
netstat -ano | findstr :8770

# æ€æ­»è¿›ç¨‹
taskkill /F /PID <PID>

# æˆ–è€…ä½¿ç”¨å…¶ä»–ç«¯å£
python server_minimal.py --port 8771
```

### é—®é¢˜ 2ï¼šKuzuDB åˆå§‹åŒ–å¤±è´¥

**é”™è¯¯ï¼š**
```
Exception: Failed to open database
```

**è§£å†³ï¼š**
```bash
# åˆ é™¤æŸåçš„æ•°æ®åº“
rm -rf backend/database/twitter_scraper

# é‡å¯åç«¯ä¼šè‡ªåŠ¨åˆ›å»º
python server_minimal.py --port 8770
```

### é—®é¢˜ 3ï¼šä¾èµ–å®‰è£…å¤±è´¥

**é”™è¯¯ï¼š**
```
ERROR: Could not find a version that satisfies the requirement kuzu
```

**è§£å†³ï¼š**
```bash
# å‡çº§ pip
python -m pip install --upgrade pip

# é‡æ–°å®‰è£…
pip install -r requirements.txt

# æˆ–è€…ä½¿ç”¨å›½å†…é•œåƒ
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡æ“ä½œ

```python
# å¥½çš„åšæ³•ï¼šæ‰¹é‡æ’å…¥
await kg.add_posts_batch(posts)

# ä¸å¥½çš„åšæ³•ï¼šé€æ¡æ’å…¥
for post in posts:
    await kg.add_post(post)
```

### ç´¢å¼•ä¼˜åŒ–

KuzuDB è‡ªåŠ¨ä¸º PRIMARY KEY åˆ›å»ºç´¢å¼•ã€‚

å¦‚éœ€æ·»åŠ æ—¶é—´æˆ³ç´¢å¼•ï¼š
```python
# åœ¨ schema ä¸­æ·»åŠ 
CREATE NODE INDEX ON Post(scrapedAt)
```

### è¿æ¥ç®¡ç†

```python
# æ­£ç¡®ï¼šä½¿ç”¨å®Œå…³é—­è¿æ¥
kg = SocialScraperKG(db_path)
await kg.init()
# ... ä½¿ç”¨ ...
await kg.close()

# é”™è¯¯ï¼šä¸å…³é—­è¿æ¥
kg = SocialScraperKG(db_path)
await kg.init()
# ... ä½¿ç”¨åä¸å…³é—­
```

---

## ğŸ“„ æ–‡ä»¶è¯´æ˜

```
backend/
â”œâ”€â”€ server_minimal.py     # æç®€ HTTP æœåŠ¡å™¨ï¼ˆ150 è¡Œï¼‰
â”œâ”€â”€ database.py           # KuzuDB æ•°æ®è®¿é—®å±‚ï¼ˆ600 è¡Œï¼‰
â”œâ”€â”€ requirements.txt      # Python ä¾èµ–ï¼ˆä»… kuzuï¼‰
â”œâ”€â”€ TEST_REPORT.md        # åç«¯æµ‹è¯•æŠ¥å‘Š
â””â”€â”€ database/             # æ•°æ®åº“ç›®å½•
    â””â”€â”€ twitter_scraper/  # KuzuDB æ–‡ä»¶
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§éƒ¨ç½²

```bash
# ä½¿ç”¨ systemd ç®¡ç†ï¼ˆLinuxï¼‰
sudo systemctl add twitter-scraper-backend

# ä½¿ç”¨ PM2 ç®¡ç†
pm2 start server_minimal.py --name twitter-backend

# ä½¿ç”¨ Docker éƒ¨ç½²
docker-compose up -d
```

### 2. ç›‘æ§å‘Šè­¦

æ·»åŠ å¥åº·æ£€æŸ¥è„šæœ¬ï¼š
```bash
#!/bin/bash
# check_health.sh
curl -f http://localhost:8770/health || exit 1
```

### 3. å®šæœŸå¤‡ä»½

```bash
# crontab ç¤ºä¾‹
# æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½
0 2 * * * cp -r /path/to/database /backup/$(date +\%Y\%m\%d)
```

---

## ğŸ“ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šä¸ªäººç ”ç©¶

```bash
# æœ¬åœ°è¿è¡Œï¼Œä»…è‡ªå·±ä½¿ç”¨
python server_minimal.py --port 8770
```

### åœºæ™¯ 2ï¼šå°å›¢é˜Ÿå…±äº«

```bash
# å±€åŸŸç½‘å†…å…±äº«
python server_minimal.py --host 0.0.0.0 --port 8770
```

å›¢é˜Ÿæˆå‘˜ Chrome æ‰©å±•é…ç½®ï¼š
```typescript
const BACKEND_HOST = 'http://192.168.1.100:8770'
```

### åœºæ™¯ 3ï¼šæœåŠ¡å™¨éƒ¨ç½²

```bash
# ä½¿ç”¨ Nginx åå‘ä»£ç†
# é…ç½® SSL è¯ä¹¦
# æ·»åŠ è®¤è¯æœºåˆ¶
```

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** 1.0  
**æœ€åæ›´æ–°ï¼š** 2026-02-25  
**ç»´æŠ¤è€…ï¼š** Twitter Scraper Team
